"""
InternTA: Synthetic Biology Teaching Assistant
==============================================

This application provides a conversational AI assistant specialized in synthetic biology education.
It supports two operation modes:
1. Remote API mode - Connects to a remote API endpoint (api.ecopi.chat) using an API key
2. Local model mode - Loads and runs a fine-tuned language model locally (if available)

Features:
- Interactive chat interface with Streamlit
- Support for streaming responses (local model only)
- LaTeX equation rendering in markdown
- Special handling for thinking/reasoning sections with </think> tags
- Configurable generation parameters (temperature, top_p, etc.)
- GPU memory management for local model operation

Usage:
- Run with `--local` flag to default to local model mode
- Run with `--show-local-option` to allow users to switch between local and remote modes

Dependencies:
- streamlit for the web interface
- transformers, torch, and peft for local model loading and inference
- requests for API communication

"""

import streamlit as st
import requests
import json
import os
import re
from datetime import datetime
import sys
import asyncio
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel  # Import PeftModel for loading LoRA adapters
import time
import argparse  # Add argparse for command line arguments


# Function to process content for display
def process_content_for_display(content, is_user_message=False):
    """
    Process content for display with special formatting for </think> tags,
    strip <|im_end|> tokens and backslashes, and ensure LaTeX equations display correctly
    """
    if not content:
        return ""
    
    # Strip <|im_end|> tokens
    content = content.replace("<|im_end|>", "")
    
    # Replace double backslashes with single backslashes
    if is_user_message:
        content = content.replace("\\\\", "\\")
    
    # Store all LaTeX expressions temporarily to preserve them
    latex_expressions = []
    
    # Function to replace LaTeX with placeholders
    def store_latex(match):
        latex = match.group(0)  # Get the entire match including delimiters
        placeholder = f"LATEX_PLACEHOLDER_{len(latex_expressions)}"
        latex_expressions.append(latex)
        return placeholder
    
    # Process different LaTeX patterns:
    # 1. Single dollar sign $...$
    content = re.sub(r'\$(.*?)\$', store_latex, content)
    
    # 2. Double dollar sign $$...$$
    content = re.sub(r'\$\$(.*?)\$\$', store_latex, content)
    
    # 3. \(...\) notation
    content = re.sub(r'\\\((.*?)\\\)', store_latex, content)
    
    # 4. \[...\] notation
    content = re.sub(r'\\\[(.*?)\\\]', store_latex, content)

    # 5. [...] notation
    content = re.sub(r'\[(.*?)\]', store_latex, content)
    
    # Restore LaTeX expressions with proper formatting for Streamlit markdown
    for i, latex in enumerate(latex_expressions):
        placeholder = f"LATEX_PLACEHOLDER_{i}"
        
        # Convert \(...\) to $...$ format
        if latex.startswith('\\(') and latex.endswith('\\)'):
            inner_content = latex[2:-2]  # Remove \( and \)
            latex = f'${inner_content}$'
        
        # Convert \[...\] to $$...$$ format
        elif latex.startswith('\\[') and latex.endswith('\\]'):
            inner_content = latex[2:-2]  # Remove \[ and \]
            latex = f'$${inner_content}$$'

        # Convert [...] to $...$ format
        elif latex.startswith('[') and latex.endswith(']'):
            inner_content = latex[2:-2]  # Remove [ and ]
            latex = f'${inner_content}$'
        
        # Replace the placeholder with the properly formatted LaTeX
        content = content.replace(placeholder, latex)
    
    # Find the position of the </think> tag
    think_pos = content.find('</think>')
    
    if think_pos == -1:
        # No </think> tag found, return content as is
        return content
    
    # Split the content into thinking part and regular part
    thinking_part = content[:think_pos + len('</think>')]
    regular_part = content[think_pos + len('</think>'):]
    
    # Apply styling to the thinking part using HTML with explicit dark font color for dark mode compatibility
    styled_thinking_part = f'''<div style="background-color: #f0f7ff; 
                                     border-left: 3px solid #4a88e5; 
                                     padding: 10px; 
                                     margin-bottom: 10px; 
                                     border-radius: 5px; 
                                     font-style: italic;
                                     color: #333333;">
                                {thinking_part}
                              </div>'''
    
    # Combine the styled thinking part with the regular part
    return styled_thinking_part + regular_part

# Function to load local model
@st.cache_resource
def load_local_model(lora_adapter_path="internTAv2.0_test", base_model_path="DeepSeek-R1-Distill-Qwen-7B"):
    """Load the base model and LoRA adapter separately, then merge them in memory"""
    print(f"Loading base model from {base_model_path} and LoRA adapter from {lora_adapter_path}...")
    
    # Check if CUDA is available and set the device accordingly
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")
    
    # Setup quantization config for 4-bit if using CUDA
    if device == "cuda":
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
        )
        
        # Load the base model with quantization
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            quantization_config=bnb_config,  # 4-bit quantization
            device_map="auto",
            trust_remote_code=True
        )
    else:
        # Load the base model without quantization for CPU
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            device_map="auto",
            trust_remote_code=True
        )
    
    # Load the LoRA adapter and apply it to the base model
    try:
        print(f"Applying LoRA adapter from {lora_adapter_path} to base model...")
        lora_model = PeftModel.from_pretrained(base_model, lora_adapter_path)
        print("LoRA adapter applied successfully!")
        
        # Load tokenizer from the adapter path (it contains the specific tokenizer settings for the model)
        tokenizer = AutoTokenizer.from_pretrained(lora_adapter_path)
        
        # Fix for attention mask warning - ensure pad_token is properly set
        if tokenizer.pad_token is None:
            if tokenizer.eos_token is not None:
                tokenizer.pad_token = tokenizer.eos_token
            else:
                # Add a new pad token if neither pad nor eos token exists
                tokenizer.add_special_tokens({'pad_token': '[PAD]'})
                # Resize model embeddings to match the new vocabulary size
                lora_model.resize_token_embeddings(len(tokenizer))
        
        print("Model loaded successfully!")
        return lora_model, tokenizer
    except Exception as e:
        print(f"Error applying LoRA adapter: {str(e)}")
        # Clean up resources
        del base_model
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        raise e

# Function to clear CUDA cache safely
def clear_cuda_cache():
    """Clear CUDA cache if available"""
    if torch.cuda.is_available():
        try:
            torch.cuda.empty_cache()
            torch.cuda.synchronize()  # Wait for all CUDA operations to finish
            print("CUDA cache cleared")
            return True
        except Exception as e:
            print(f"Error clearing CUDA cache: {str(e)}")
            return False
    return False

# Function to generate streaming response from local model
def stream_generate(model, tokenizer, input_ids, attention_mask=None, **gen_kwargs):
    """Generate text in a streaming fashion using the approach from run.py"""
    try:
        # Remove attention_mask from gen_kwargs if it's there to avoid duplication
        streamer_kwargs = {k: v for k, v in gen_kwargs.items() if k != 'attention_mask'}
        
        # Get generation parameters
        max_new_tokens = streamer_kwargs.get("max_new_tokens", 1000)
        do_sample = streamer_kwargs.get("do_sample", True)
        temperature = streamer_kwargs.get("temperature", 0.2)
        top_p = streamer_kwargs.get("top_p", 0.5)
        repetition_penalty = streamer_kwargs.get("repetition_penalty", 1.0)
        pad_token_id = streamer_kwargs.get("pad_token_id", tokenizer.eos_token_id)
        eos_token_id = tokenizer.eos_token_id
        
        # Setup for generation
        unfinished_sequences = torch.ones(input_ids.shape[0], dtype=torch.long, device=input_ids.device)
        input_length = input_ids.shape[1]  # Remember initial input length
        
        # Build model kwargs for prepare_inputs_for_generation
        model_kwargs = {
            "attention_mask": attention_mask,
        }
        
        # Main generation loop
        for _ in range(max_new_tokens):
            with torch.no_grad():
                # Prepare model inputs
                model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)
                
                # Forward pass
                outputs = model(
                    **model_inputs,
                    return_dict=True,
                    output_attentions=False,
                    output_hidden_states=False,
                )
                
                # Get next token logits
                next_token_logits = outputs.logits[:, -1, :]
                
                # Apply repetition penalty if needed
                if repetition_penalty > 1.0:
                    for i in range(input_ids.shape[0]):
                        for previous_token in input_ids[i]:
                            # Exponential penalty
                            next_token_logits[i, previous_token] /= repetition_penalty
                
                # Apply temperature
                if temperature > 0:
                    next_token_logits = next_token_logits / temperature
                
                # Filter with top_p
                if top_p < 1.0 and top_p > 0:
                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                    cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)
                    
                    # Remove tokens with cumulative probability above the threshold
                    sorted_indices_to_remove = cumulative_probs > top_p
                    # Shift the indices to the right to keep also the first token above the threshold
                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                    sorted_indices_to_remove[..., 0] = 0
                    
                    for i in range(next_token_logits.shape[0]):
                        indices_to_remove = sorted_indices[i][sorted_indices_to_remove[i]]
                        next_token_logits[i, indices_to_remove] = -float("Inf")
                
                # Sample or greedy select
                if do_sample:
                    probs = torch.nn.functional.softmax(next_token_logits, dim=-1)
                    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
                else:
                    next_tokens = torch.argmax(next_token_logits, dim=-1)
                
                # Append next tokens to input_ids
                input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)
                
                # Update model kwargs for next generation step
                if attention_mask is not None:
                    # Extend attention mask for new tokens
                    attention_mask = torch.cat([
                        attention_mask, 
                        torch.ones((attention_mask.shape[0], 1), dtype=attention_mask.dtype, device=attention_mask.device)
                    ], dim=1)
                
                # Update model_kwargs with updated attention_mask
                model_kwargs["attention_mask"] = attention_mask
                
                # Check if any sequences are finished
                unfinished_sequences = unfinished_sequences.mul((next_tokens != eos_token_id).long())
                
                # Decode the generated tokens so far (skip initial input)
                output_ids = input_ids[0, input_length:].cpu().tolist()
                text_generated = tokenizer.decode(output_ids, skip_special_tokens=True)
                
                # Free GPU memory
                del next_token_logits, next_tokens
                if 'sorted_logits' in locals(): del sorted_logits
                if 'sorted_indices' in locals(): del sorted_indices
                if 'cumulative_probs' in locals(): del cumulative_probs
                if 'probs' in locals(): del probs
                
                # Yield the generated text
                yield text_generated
                
                # Exit if all sequences are finished
                if unfinished_sequences.max() == 0:
                    break
    finally:
        # Always clean up CUDA memory after generation, even if there's an error
        del model_inputs, outputs
        if 'next_token_logits' in locals(): del next_token_logits
        if 'next_tokens' in locals(): del next_tokens
        
        # Optional additional cleanup for CUDA memory
        #clear_cuda_cache()

# Function to generate response using local model
def generate_local_response(model, tokenizer, messages, temperature=0.2, top_p=0.15, 
                           repetition_penalty=1.05, max_tokens=4096, do_sample=True, stream=True):
    """Generate a response using the local model with optional streaming"""
    try:
        # Format the conversation history for the model
        prompt = ""
        for msg in messages:
            role = "user" if msg["role"] == "user" else "assistant"
            prompt += f"{role}: {msg['content']}\n"
        
        prompt += "assistant: "
        
        # Tokenize the prompt with explicit attention mask
        tokenized_input = tokenizer(
            prompt, 
            return_tensors="pt",
            padding=True,
            return_attention_mask=True
        )
        input_ids = tokenized_input["input_ids"].to(model.device)
        attention_mask = tokenized_input["attention_mask"].to(model.device)
        
        # Set generation parameters - use all parameters from the UI
        gen_config = {
            "max_new_tokens": max_tokens,
            "temperature": temperature,
            "do_sample": do_sample,  # Use the parameter from UI
            "top_p": top_p,  # Use the parameter from UI
            "repetition_penalty": repetition_penalty,  # Use the parameter from UI
            "pad_token_id": tokenizer.eos_token_id
        }
        
        if stream:
            # Streaming generation
            response = ""
            for new_text in stream_generate(model, tokenizer, input_ids, attention_mask=attention_mask, **gen_config):
                response = new_text
                yield response
        else:
            # Non-streaming generation
            outputs = model.generate(
                input_ids,
                attention_mask=attention_mask,
                **gen_config
            )
            response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)
            yield response
    finally:
        # Always clean up to avoid memory leaks
        if 'tokenized_input' in locals(): del tokenized_input
        if 'input_ids' in locals(): del input_ids
        if 'attention_mask' in locals(): del attention_mask
        if 'outputs' in locals(): del outputs
        
        # Final CUDA cleanup
        #clear_cuda_cache()

# Setup command line arguments and default model source
def parse_args():
    parser = argparse.ArgumentParser(description="InternTA: Synthetic Biology Teaching Assistant")
    parser.add_argument("--local", action="store_true", help="Use local model instead of remote API")
    parser.add_argument("--show-local-option", action="store_true", help="Show local model as an option in the UI")
    return parser.parse_args()

# Main function to start the app
def main():
    # Parse command line arguments
    args = parse_args()
    
    # Set default model source based on command line arguments
    if "model_source" not in st.session_state:
        if args.local:
            st.session_state.model_source = "æœ¬åœ°æ¨¡å‹ | Local Model"
        else:
            st.session_state.model_source = "è¿œç¨‹ API | Remote API"
    
    # Store show_local_option flag in session state
    if "show_local_option" not in st.session_state:
        st.session_state.show_local_option = args.show_local_option

    # Start the Streamlit app (the existing code continues from here)
    # Page configuration
    st.set_page_config(
        page_title="InternTA: åˆæˆç”Ÿç‰©å­¦åŠ©æ•™ | Synthetic Biology Teaching Assistant",
        page_icon="ğŸ§¬",
        layout="wide"
    )

    # Initialize messages if not in session state
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Fixed API URL - no need for user input
    API_BASE_URL = "https://api.ecopi.chat/v1/chat/completions"

    if "api_key" not in st.session_state:
        st.session_state.api_key = ""

    # Fixed model name - no need for user input
    MODEL_NAME = "internta"

    # Sidebar for configuration
    with st.sidebar:
        st.title("InternTA: åˆæˆç”Ÿç‰©å­¦åŠ©æ•™ | Synthetic Biology TA")

        # Model source selection - depends on whether to show local option
        if st.session_state.show_local_option:
            # Show both options
            model_source = st.radio(
                "æ¨¡å‹æ¥æº | Model Source",
                ["æœ¬åœ°æ¨¡å‹ | Local Model", "è¿œç¨‹ API | Remote API"],
                index=0 if st.session_state.model_source == "æœ¬åœ°æ¨¡å‹ | Local Model" else 1
            )
        else:
            # Only show remote API option
            model_source = "è¿œç¨‹ API | Remote API"
            
        # Update session state if user changes selection
        if model_source != st.session_state.model_source:
            st.session_state.model_source = model_source
        
        if model_source == "è¿œç¨‹ API | Remote API":
            # Display the fixed API URL (read-only)
            st.info(f"è·å– API å¯†é’¥åœ°å€ | Obtain API Key from here: https://docs.ecopi.chat")
            
            # API Key input
            api_key = st.text_input("Please enter an API Key | è¯·è¾“å…¥ API å¯†é’¥", 
                                   value=st.session_state.api_key,
                                   placeholder="sk-...",
                                   type="password")
            
            if api_key != st.session_state.api_key:
                st.session_state.api_key = api_key
            
            # è¿œç¨‹APIæ¨¡å¼ä¸‹çš„å‚æ•°è®¾ç½®
            st.write("å‚æ•°è®¾ç½® | Parameters:")
            temperature = st.slider("æ¸©åº¦ | Temperature", min_value=0.0, max_value=1.0, value=0.2, step=0.05)
            top_p = st.slider("Top P (æ ¸é‡‡æ ·é˜ˆå€¼ | Nucleus sampling threshold)", min_value=0.0, max_value=1.0, value=0.15, step=0.05)
            max_tokens = st.number_input("æœ€å¤§ç”Ÿæˆé•¿åº¦ | Max Tokens", min_value=1, max_value=100000, value=4096, step=100)
            
            # æ˜¾ç¤ºæµå¼å“åº”ä¸å¯ç”¨çš„æç¤º
            st.warning("è¿œç¨‹APIä¸æ”¯æŒæµå¼å“åº”ï¼Œå°†ä½¿ç”¨éæµå¼æ¨¡å¼ | Remote API does not support streaming, will use non-streaming mode")
            use_streaming = False  # è¿œç¨‹APIå¼ºåˆ¶éæµå¼
        else:
            # Path to base model and LoRA adapter
            with st.expander("æ¨¡å‹è®¾ç½® | Model Settings", expanded=True):
                base_model_path = st.text_input(
                    "åŸºç¡€æ¨¡å‹è·¯å¾„ | Base Model Path",
                    value="DeepSeek-R1-Distill-Qwen-7B",
                    help="åŸºç¡€å¤§è¯­è¨€æ¨¡å‹çš„è·¯å¾„ | Path to base language model"
                )
                
                lora_adapter_path = st.text_input(
                    "LoRAé€‚é…å™¨è·¯å¾„ | LoRA Adapter Path",
                    value="internTAv2.0_test",
                    help="LoRAå¾®è°ƒé€‚é…å™¨çš„è·¯å¾„ | Path to LoRA fine-tuned adapter"
                )
            
            # Check if we need to load or reload the model
            model_changed = (
                "base_model_path" not in st.session_state or 
                "lora_adapter_path" not in st.session_state or
                st.session_state.get("base_model_path", "") != base_model_path or
                st.session_state.get("lora_adapter_path", "") != lora_adapter_path
            )
            
            if model_changed or "local_model" not in st.session_state:
                # Only load the model if it's not loaded or the paths have changed
                with st.spinner("åŠ è½½æ¨¡å‹ä¸­... | Loading model..."):
                    try:
                        st.session_state.local_model, st.session_state.local_tokenizer = load_local_model(
                            lora_adapter_path=lora_adapter_path,
                            base_model_path=base_model_path
                        )
                        # Store current paths in session state
                        st.session_state.base_model_path = base_model_path
                        st.session_state.lora_adapter_path = lora_adapter_path
                        st.success("æ¨¡å‹åŠ è½½æˆåŠŸï¼| Model loaded successfully!")
                    except Exception as e:
                        st.error(f"åŠ è½½æ¨¡å‹å¤±è´¥ | Failed to load model: {str(e)}")
                        if "cuda" in str(e).lower() and "out of memory" in str(e).lower():
                            st.warning("GPUå†…å­˜ä¸è¶³ï¼Œè¯·è€ƒè™‘ä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ–æ¸…ç†GPUå†…å­˜ | GPU out of memory, consider using a smaller model or freeing GPU memory")
            
            # æœ¬åœ°æ¨¡å‹çš„å‚æ•°è®¾ç½® - Enhanced with parameters from run.py
            st.write("ç”Ÿæˆå‚æ•°è®¾ç½® | Generation Parameters:")
            temperature = st.slider("æ¸©åº¦ | Temperature", min_value=0.0, max_value=1.0, value=0.2, step=0.05, 
                                  help="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ã€‚è¾ƒé«˜çš„å€¼ (å¦‚ 0.8) ä¼šä½¿è¾“å‡ºæ›´åŠ å¤šæ ·åŒ–ï¼Œè¾ƒä½çš„å€¼ (å¦‚ 0.2) ä½¿è¾“å‡ºæ›´åŠ ç¡®å®šå’Œé›†ä¸­ | Controls randomness in generation. Higher (0.8) is more diverse, lower (0.2) more focused")
            
            top_p = st.slider("Top P", min_value=0.0, max_value=1.0, value=0.15, step=0.05,
                             help="æ ¸é‡‡æ ·é˜ˆå€¼ - æ¨¡å‹åªè€ƒè™‘æ¦‚ç‡æ€»å’Œè¾¾åˆ°æ­¤å€¼çš„å€™é€‰è¯ | Nucleus sampling threshold - model only considers tokens that make up this probability mass")
            
            repetition_penalty = st.slider("é‡å¤æƒ©ç½š | Repetition Penalty", min_value=1.0, max_value=2.0, value=1.05, step=0.05,
                                         help="æ§åˆ¶é‡å¤å†…å®¹çš„æƒ©ç½šåŠ›åº¦ï¼Œè¾ƒé«˜çš„å€¼ä¼šå‡å°‘é‡å¤ | Controls penalty for repetition, higher reduces repetition")
            
            max_tokens = st.number_input("æœ€å¤§ç”Ÿæˆé•¿åº¦ | Max Tokens", min_value=100, max_value=100000, value=4096, step=100,
                                       help="å“åº”ä¸­ç”Ÿæˆçš„æœ€å¤§æ ‡è®°æ•° | Maximum number of tokens to generate in response")
            
            # é«˜çº§é€‰é¡¹ (å¯æŠ˜å ) | Advanced options (collapsible)
            with st.expander("é«˜çº§é€‰é¡¹ | Advanced Options"):
                do_sample = st.checkbox("ä½¿ç”¨é‡‡æ · | Use Sampling", value=True, 
                                      help="å¯ç”¨ä»æ¦‚ç‡åˆ†å¸ƒé‡‡æ ·ï¼Œå…³é—­åˆ™ä½¿ç”¨è´ªå©ªæœç´¢ | Enable sampling from probability distribution, disable for greedy search")
                
            # æœ¬åœ°æ¨¡å‹æ”¯æŒæµå¼å“åº”é€‰é¡¹
            use_streaming = st.checkbox("æµå¼å“åº” | Streaming Response", value=True, 
                                      help="é€æ­¥æ˜¾ç¤ºç”Ÿæˆçš„æ–‡æœ¬ | Display generated text incrementally")
        
        # Memory management
        with st.expander("å†…å­˜ç®¡ç† | Memory Management"):
            if st.button("æ¸…ç†GPUç¼“å­˜ | Clear GPU Cache"):
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    st.success("GPUç¼“å­˜å·²æ¸…ç† | GPU cache cleared")
                else:
                    st.info("æœªæ£€æµ‹åˆ°GPU | No GPU detected")
        
        # Clear chat button
        if st.button("æ¸…ç©ºå¯¹è¯ | Clear Chat", help="æ¸…é™¤æ‰€æœ‰å¯¹è¯å†å² | Clear all chat history"):
            st.session_state.messages = []
            st.rerun()

    # Main chat interface
    #st.title("InternTA: åˆæˆç”Ÿç‰©å­¦åŠ©æ•™ | Synthetic Biology Teaching Assistant")
    #st.caption("åŸºäº InternLM2 å¤§æ¨¡å‹ï¼Œå¸®åŠ©å­¦ç”Ÿæ›´å¥½åœ°å­¦ä¹ ã€Šåˆæˆç”Ÿç‰©å­¦ã€‹ | Based on InternLM2 model, helping students better learn Synthetic Biology")

    # Display chat messages
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            # Process the content for display with all necessary transformations
            processed_content = process_content_for_display(message["content"])
            st.markdown(processed_content, unsafe_allow_html=True)

    # Get user input
    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜... | Enter your question..."):
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # Display user message
        with st.chat_message("user"):
            processed_prompt = process_content_for_display(prompt, is_user_message=True)
            st.markdown(processed_prompt, unsafe_allow_html=True)
        
        # Prepare model call
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            message_placeholder.markdown("æ€è€ƒä¸­... | Thinking...")
            
            try:
                # Determine which model to use
                if model_source == "æœ¬åœ°æ¨¡å‹ | Local Model":
                    # Check if model is loaded
                    if "local_model" not in st.session_state:
                        message_placeholder.error("æœ¬åœ°æ¨¡å‹æœªåŠ è½½ï¼Œè¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„ | Local model not loaded, please check model path")
                        st.stop()
                    
                    # Use local model
                    if use_streaming:
                        # Process streaming response
                        full_response = ""
                        
                        # Stream the response with all parameters from UI
                        for response_chunk in generate_local_response(
                            st.session_state.local_model,
                            st.session_state.local_tokenizer,
                            st.session_state.messages,
                            temperature=temperature,
                            top_p=top_p,
                            repetition_penalty=repetition_penalty,
                            max_tokens=max_tokens,
                            do_sample=do_sample if 'do_sample' in locals() else True,
                            stream=True
                        ):
                            # Update the full response
                            full_response = response_chunk
                            # Update the placeholder with the processed content
                            processed_content = process_content_for_display(full_response)
                            message_placeholder.markdown(processed_content + "â–Œ", unsafe_allow_html=True)
                            # Short sleep to reduce CPU usage and improve UI responsiveness
                            time.sleep(0.01)
                        
                        # Final update without the cursor
                        if full_response:
                            processed_content = process_content_for_display(full_response)
                            message_placeholder.markdown(processed_content, unsafe_allow_html=True)
                            # Add assistant response to chat history
                            st.session_state.messages.append({"role": "assistant", "content": full_response})
                        else:
                            message_placeholder.error("æ— å“åº”å†…å®¹ | No response content")
                    else:
                        # Non-streaming generation with all parameters from UI
                        full_response = ""
                        for response in generate_local_response(
                            st.session_state.local_model,
                            st.session_state.local_tokenizer,
                            st.session_state.messages,
                            temperature=temperature,
                            top_p=top_p,
                            repetition_penalty=repetition_penalty,
                            max_tokens=max_tokens,
                            do_sample=do_sample if 'do_sample' in locals() else True,
                            stream=False
                        ):
                            full_response = response
                            break  # Only need the first (and only) result
                        
                        if full_response:
                            processed_content = process_content_for_display(full_response)
                            message_placeholder.markdown(processed_content, unsafe_allow_html=True)
                            # Add assistant response to chat history
                            st.session_state.messages.append({"role": "assistant", "content": full_response})
                        else:
                            message_placeholder.error("æ— å“åº”å†…å®¹ | No response content")
                else:
                    # Use remote API
                    if not st.session_state.api_key:
                        message_placeholder.error("è¯·åœ¨ä¾§è¾¹æ è¾“å…¥ API å¯†é’¥ | Please enter an API key in the sidebar")
                        st.stop()
                    
                    # è¿œç¨‹APIæ¨¡å¼ä¸‹å¼ºåˆ¶ä½¿ç”¨éæµå¼å“åº”ï¼Œä¸ç®¡ç”¨æˆ·åœ¨ç•Œé¢ä¸Šé€‰æ‹©ä»€ä¹ˆ
                    # For remote API, always force non-streaming mode regardless of UI selection
                    actual_streaming = False  # å¼ºåˆ¶è®¾ç½®ä¸ºFalse | Force to False
                    
                    if use_streaming:
                        message_placeholder.warning("è¿œç¨‹APIä¸æ”¯æŒæµå¼å“åº”ï¼Œå·²è‡ªåŠ¨åˆ‡æ¢ä¸ºéæµå¼æ¨¡å¼ | Remote API doesn't support streaming, automatically switched to non-streaming mode")
                    
                    # Prepare the payload for the API call
                    payload = {
                        "model": MODEL_NAME,
                        "messages": [{"role": m["role"], "content": m["content"]} for m in st.session_state.messages],
                        "temperature": float(temperature),
                        "max_tokens": int(max_tokens),
                        "top_p": float(top_p),
                        "stream": actual_streaming  # ä½¿ç”¨å¼ºåˆ¶çš„éæµå¼æ¨¡å¼ | Use forced non-streaming mode
                    }
                    
                    # Add optional parameters if provided from UI
                    if "repetition_penalty" in locals() and repetition_penalty > 1.0:
                        payload["repetition_penalty"] = float(repetition_penalty)
                    
                    # If there's an n parameter (number of completions), ensure it's an integer
                    payload["n"] = 1  # Set to 1 as we just want a single response
                    
                    headers = {
                        "Content-Type": "application/json"
                    }
                    
                    # Add Authorization header if API key is provided
                    headers["Authorization"] = f"Bearer {st.session_state.api_key}"
                    
                    # Make the API call
                    response = requests.post(
                        API_BASE_URL,
                        headers=headers,
                        json=payload,
                        stream=actual_streaming,  # ä½¿ç”¨å¼ºåˆ¶çš„éæµå¼æ¨¡å¼ | Use forced non-streaming mode
                        timeout=600  # 10-minute timeout
                    )
                    
                    if response.status_code == 200:
                        # Since we've forced non-streaming mode, we only need to handle non-streaming responses
                        try:
                            response_json = response.json()
                            if "choices" in response_json and len(response_json["choices"]) > 0:
                                content = response_json["choices"][0]["message"]["content"]
                                processed_content = process_content_for_display(content)
                                message_placeholder.markdown(processed_content, unsafe_allow_html=True)
                                # Add assistant response to chat history
                                st.session_state.messages.append({"role": "assistant", "content": content})
                            else:
                                message_placeholder.error("æ— å“åº”å†…å®¹ | No response content")
                        except Exception as e:
                            message_placeholder.error(f"å¤„ç†å“åº”æ—¶å‡ºé”™ | Error processing response: {str(e)}")
                            print(f"Response processing error: {str(e)}")
                            print(f"Response content: {response.text[:500]}")  # Print first 500 chars of response for debugging
                    else:
                        error_message = f"é”™è¯¯ | Error: {response.status_code} - {response.text}"
                        message_placeholder.error(error_message)
                        print(f"API Error: {error_message}")
                        
                        # If it's the specific "ids" error, suggest a solution
                        if "ids" in response.text and "list" in response.text and "integer" in response.text:
                            message_placeholder.warning("å°è¯•å…³é—­æµå¼å“åº”å¹¶é‡è¯• | Try turning off streaming response and retry")
                        
                        # If it's a streaming-related error, inform the user again
                        if "stream" in response.text.lower() or "not allowed" in response.text.lower():
                            message_placeholder.warning("è¿œç¨‹APIä¸æ”¯æŒæµå¼å“åº”ï¼Œè¯·ç¡®ä¿è¯·æ±‚è®¾ç½®ä¸ºéæµå¼æ¨¡å¼ | Remote API does not support streaming, please ensure requests are set to non-streaming mode")
                
                # Clear GPU cache if using CUDA
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
            except Exception as e:
                message_placeholder.error(f"é”™è¯¯ | Error: {str(e)}")
                print(f"Generation error: {str(e)}")
                
                # If it's a CUDA out of memory error, suggest a solution
                if "CUDA out of memory" in str(e):
                    message_placeholder.warning("GPUå†…å­˜ä¸è¶³ï¼Œè¯·å°è¯•é™ä½æœ€å¤§ç”Ÿæˆé•¿åº¦æˆ–ä½¿ç”¨CPUæ¨¡å¼ | GPU out of memory, try reducing max tokens or using CPU mode")
                    
                # Clear GPU cache if using CUDA
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

    # Display some helpful information at the bottom
    st.markdown("---")
    if model_source == "æœ¬åœ°æ¨¡å‹ | Local Model":
        st.caption(f"ä½¿ç”¨æ¨¡å‹ | Using model: Base={base_model_path}, LoRA={lora_adapter_path}")
    else:
        st.caption("é»˜è®¤ API ç«¯ç‚¹ ï½œ Default API Endpoint: https://api.ecopi.chat - è¯·åœ¨ä¾§è¾¹æ è¾“å…¥ API å¯†é’¥å¼€å§‹å¯¹è¯ã€‚| Enter your API key in the sidebar to begin.")

# Run the main function when executed directly
if __name__ == "__main__":
    main()
